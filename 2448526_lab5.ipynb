{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMESyMPIrqPZw/VmCaP7vUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinijalanda93/Predictive_Analystics/blob/main/2448526_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Define the environment (4x4 Grid)\n",
        "# 0 - Free cell, 1 - Obstacle, 2 - Goal\n",
        "env = np.array([\n",
        "    [0, 0, 0, 0],\n",
        "    [0, 1, 0, 1],\n",
        "    [0, 0, 0, 0],\n",
        "    [1, 0, 2, 0]\n",
        "])\n",
        "\n",
        "n_states = env.shape[0] * env.shape[1]\n",
        "n_actions = 4  # Up, Down, Left, Right\n",
        "\n",
        "# Q-table initialization\n",
        "Q = np.zeros((n_states, n_actions))"
      ],
      "metadata": {
        "id": "hH6FFgg4KAfm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning parameters\n",
        "alpha = 0.8      # Learning rate\n",
        "gamma = 0.9      # Discount factor\n",
        "epsilon = 0.1    # Exploration rate\n",
        "episodes = 500   # Training episodes"
      ],
      "metadata": {
        "id": "F6Rkl6p0KEb6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "def state_to_index(state):\n",
        "    \"\"\"Convert (row, col) to single index\"\"\"\n",
        "    return state[0] * env.shape[1] + state[1]\n",
        "\n",
        "def index_to_state(index):\n",
        "    \"\"\"Convert index back to (row, col)\"\"\"\n",
        "    return (index // env.shape[1], index % env.shape[1])\n",
        "\n",
        "def get_next_state(state, action):\n",
        "    \"\"\"Get next state after taking action\"\"\"\n",
        "    row, col = state\n",
        "    if action == 0 and row > 0: row -= 1       # Up\n",
        "    elif action == 1 and row < env.shape[0]-1: row += 1  # Down\n",
        "    elif action == 2 and col > 0: col -= 1     # Left\n",
        "    elif action == 3 and col < env.shape[1]-1: col += 1  # Right\n",
        "    return (row, col)"
      ],
      "metadata": {
        "id": "BdZtRMpuKN-E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z_-HAyIJs8J",
        "outputId": "7c13cff6-2a6d-4690-be1e-3812a0ece22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n",
            "\n",
            "Final Q-Table:\n",
            "[[ 1.81  3.12  1.81  0.63]\n",
            " [ 0.06 -4.    1.81 -2.1 ]\n",
            " [-2.21 -1.71 -1.77 -2.1 ]\n",
            " [-2.21 -4.   -2.14 -2.21]\n",
            " [ 1.81  4.58  3.12  0.56]\n",
            " [-0.96  6.2   3.12 -1.65]\n",
            " [-2.32  8.   -4.   -4.  ]\n",
            " [-0.8  -0.8  -0.8  -4.  ]\n",
            " [ 3.12 -0.88  4.58  6.2 ]\n",
            " [ 0.46  8.    4.58  8.  ]\n",
            " [ 6.2  10.    6.2   6.2 ]\n",
            " [-4.   -0.8   8.    0.  ]\n",
            " [ 4.58 -1.5  -4.   -0.8 ]\n",
            " [-0.8   6.24 -4.58 10.  ]\n",
            " [ 0.    0.    0.    0.  ]\n",
            " [-0.8   0.    0.    0.  ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def get_reward(state):\n",
        "    \"\"\"Reward function\"\"\"\n",
        "    if env[state] == 2:\n",
        "        return 10  # Goal\n",
        "    elif env[state] == 1:\n",
        "        return -5  # Obstacle\n",
        "    else:\n",
        "        return -1  # Normal step\n",
        "\n",
        "# Training\n",
        "for episode in range(episodes):\n",
        "    state = (0, 0)  # start state\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        s_idx = state_to_index(state)\n",
        "\n",
        "        # Îµ-greedy policy\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, n_actions-1)\n",
        "        else:\n",
        "            action = np.argmax(Q[s_idx, :])\n",
        "\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = get_reward(next_state)\n",
        "        next_idx = state_to_index(next_state)\n",
        "\n",
        "        # Q-learning update rule\n",
        "        Q[s_idx, action] = Q[s_idx, action] + alpha * (reward + gamma * np.max(Q[next_idx, :]) - Q[s_idx, action])\n",
        "\n",
        "        # Check if goal reached\n",
        "        if env[next_state] == 2:\n",
        "            done = True\n",
        "        state = next_state\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(\"\\nFinal Q-Table:\")\n",
        "print(Q.round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBqkS_PeKRTx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}